# Hand Gesture Recognition Model

This project aims to develop a hand gesture recognition model capable of accurately identifying and classifying different hand gestures from image or video data. Such a model can enable intuitive human-computer interaction and gesture-based control systems.

## Project Overview

The goal is to build a machine learning model that can recognize and classify hand gestures in real-time or from static images. Hand gestures can convey various commands or signals, and an accurate recognition system can facilitate natural interactions with computers, virtual reality environments, or robotics.

## Dataset

The dataset used for training and testing the hand gesture recognition model consists of labeled images or videos of different hand gestures. Each sample in the dataset is associated with a specific hand gesture class, allowing the model to learn the patterns and features associated with each gesture.

## Implementation

The model is implemented using machine learning techniques, particularly classification algorithms, trained on the labeled hand gesture dataset. The following code demonstrates how to evaluate the performance of the model using precision, recall, and F1-score:
## Explanation of the Code

### Importing Libraries

The code begins by importing necessary libraries for image processing, machine learning, and evaluation. Key libraries include:
- `os`: For interacting with the file system.
- `numpy (np)`: For numerical operations and array manipulation.
- `cv2`: OpenCV library for image processing.
- `train_test_split` from `sklearn.model_selection`: For splitting the dataset into training and testing sets.
- `to_categorical` from `tensorflow.keras.utils`: For one-hot encoding the labels.
- `ImageDataGenerator` from `tensorflow.keras.preprocessing.image`: For data augmentation.
- `Sequential`, `Conv2D`, `MaxPooling2D`, `Flatten`, `Dense`, `Dropout` from `tensorflow.keras.layers`: For defining the convolutional neural network (CNN) model architecture.
- `matplotlib.pyplot` and `seaborn` for visualization.
- `confusion_matrix` from `sklearn.metrics`: For computing the confusion matrix.
- `random`: For generating random indices for displaying random images.

### Data Loading and Preprocessing

1. **Loading Images**: Images are loaded from the specified dataset path (`dataset_path`). Each image is resized to a fixed size of 64x64 pixels and converted to grayscale.
2. **Splitting Dataset**: The dataset is split into training and testing sets using `train_test_split`.
3. **Preprocessing**: 
    - Images are normalized by dividing pixel values by 255.0 to scale them to the range [0, 1].
    - A channel dimension is added to the images to match the input shape required by the CNN model.
    - Labels are one-hot encoded using `to_categorical`.

### Data Augmentation

Data augmentation is performed using `ImageDataGenerator` to create variations of the training images. Augmentation techniques include rotation, zoom, width and height shifts, and horizontal flipping.

### CNN Model Definition and Compilation

A CNN model is defined using `Sequential` API from Keras. The model consists of convolutional layers (`Conv2D`), max pooling layers (`MaxPooling2D`), dropout layers (`Dropout`), and fully connected layers (`Dense`). The model is compiled with the Adam optimizer and categorical cross-entropy loss function.

### Model Training and Evaluation

The model is trained using `model.fit` with data generated by `datagen.flow` for data augmentation. Training progress and validation metrics are stored in `history`. The model is evaluated on the test set using `model.evaluate`, and test accuracy is printed to the console.

### Prediction and Performance Evaluation

Predictions are made on the test set using the trained model. Accuracy is calculated manually by comparing predicted and true classes. A confusion matrix is computed using `confusion_matrix` and visualized using `seaborn.heatmap`. Random images from the test set are displayed with their true and predicted labels.

### Additional Evaluation Metrics

Finally, precision, recall, and F1-score are calculated using functions from `sklearn.metrics` to further evaluate the model's performance on the test set.


## Results

The hand gesture recognition model achieved the following results:

- **Test Accuracy**: The accuracy of the model on the test set is displayed, indicating the percentage of correctly classified hand gestures.

- **Confusion Matrix**: A confusion matrix is generated and visualized using `seaborn.heatmap`, showing the true positive, false positive, true negative, and false negative predictions for each gesture class.

- **Random Image Display**: Random images from the test set are displayed along with their true and predicted labels, providing a visual representation of the model's performance on individual samples.

## Acknowledgments

The dataset used in this project is provided by Kaggle. 
